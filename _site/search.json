[
  {
    "objectID": "model222_ipyn.html",
    "href": "model222_ipyn.html",
    "title": "LSTM Spatial Flow Prediction: True Values, Predictions, and Absolute Error",
    "section": "",
    "text": "1. Data and feature construction\nWe start from a DuckDB database mobility.duckdb stored in Google Drive.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport duckdb\nimport pandas as pd\n\ncon = duckdb.connect(\"/content/drive/MyDrive/MOBILITY/mobility.duckdb\")\n\ncon.execute(\"SHOW TABLES;\").fetchdf()\n\n\n1.1 Inspecting the base table\nWe begin by examining the DuckDB database to understand the structure of the input data. First, we list all available tables and confirm that the base table flow_poi is present. We then query the schema of flow_poi to obtain the names of all its columns.\nFrom these column names, we automatically extract all features whose names start with poi_. These poi_* columns represent POI-based attributes for each grid cell (for example, counts of different POI types). Collecting them programmatically avoids hard-coding a long list of POI fields.\nFinally, we define a consistent feature ordering for later use in NumPy and PyTorch. The feature list starts with the current flow value, followed by all poi_* columns, and ends with four time-encoding features that we will add next: tod_sin, tod_cos, dow_sin, and dow_cos.\n\ninfo = con.execute(\"PRAGMA table_info('flow_poi');\").fetchall()\nall_cols = [r[1] for r in info]\npoi_cols = [c for c in all_cols if c.startswith(\"poi_\")]\n\nprint(\"lens of POI\", len(poi_cols))\nprint(\"5 poi:\", poi_cols[:5])\n\n\n\n\n1.2 Building the modeling table flow_feat\nNext, we construct a cleaned and feature-rich modeling table called flow_feat directly inside DuckDB. This table transforms the raw records from flow_poi into supervised learning examples that can be consumed by sequence models.\nThe construction proceeds in two steps. In the first step, we:\n\ncast the day (d), time (t), and grid indices (x, y) to integers;\nreplace missing values in all poi_* columns with 0, so that POI features are well-defined for every cell;\ndefine time_of_day = t as a half-hour index from 0 to 47;\ndefine dow = d % 7 as a day-of-week index from 0 to 6;\ncreate a global time_index = d * 48 + t that flattens (day, half-hour) into a single time axis.\n\nIn the second step, we enrich each row with temporal encodings and a prediction target:\n\nwe compute sinusoidal encodings of time-of-day, tod_sin and tod_cos, to represent the 48 half-hour slots within a day as points on a circle;\nwe compute sinusoidal encodings of day-of-week, dow_sin and dow_cos, to represent the weekly cycle;\nwe define flow_next as the next half-hour flow for the same grid cell, using a window function over each (x, y) time series ordered by (d, t).\n\nFinally, we select the grid indices (x, y), all model input features (flow, all poi_* columns, and the four time encodings), the target flow_next, and the global time_index. Rows where flow_next is missing (i.e., the last time step in a cell’s sequence) are dropped. The resulting flow_feat table is a clean, fully-specified dataset ready for temporal train/test splitting and sequence construction.\n\nfeature_cols = [\"flow\"] + poi_cols + [\"tod_sin\", \"tod_cos\", \"dow_sin\", \"dow_cos\"]\n\ncon.execute(f\"\"\"\n    CREATE OR REPLACE TABLE flow_feat AS\n    WITH base AS (\n        SELECT\n            d::INT AS d,\n            t::INT AS t,\n            x::INT AS x,\n            y::INT AS y,\n            flow,\n            {\", \".join([f\"COALESCE({c}, 0) AS {c}\" for c in poi_cols])},\n            t AS time_of_day,\n            d % 7 AS dow,\n            (d * 48 + t) AS time_index\n        FROM flow_poi\n    ),\n    feat AS (\n        SELECT\n            *,\n            sin(2*PI()*time_of_day/48.0) AS tod_sin,\n            cos(2*PI()*time_of_day/48.0) AS tod_cos,\n            sin(2*PI()*dow/7.0)          AS dow_sin,\n            cos(2*PI()*dow/7.0)          AS dow_cos,\n            LEAD(flow) OVER (\n                PARTITION BY x, y\n                ORDER BY d, t\n            ) AS flow_next\n        FROM base\n    )\n    SELECT\n        x,\n        y,\n        {\", \".join(feature_cols)},\n        flow_next,\n        time_index\n    FROM feat\n    WHERE flow_next IS NOT NULL;\n\"\"\")\n\n\nprint(con.execute(\"SELECT COUNT(*) FROM flow_feat;\").fetchall())\nprint(con.execute(\"PRAGMA table_info('flow_feat');\").fetchall()[:15])\n\n\n\n\n\n2. Train–test split and feature normalization\nTo prepare the data for modeling, we perform a temporal split on the feature table flow_feat. Because mobility patterns evolve over time, the model should be evaluated on future data rather than on randomly shuffled samples. We therefore split the dataset by the global time_index:\n\nWe compute the 80th percentile of time_index across all rows.\nRows with time_index ≤ threshold form the training set (flow_feat_train).\nRows with time_index &gt; threshold form the test set (flow_feat_test).\n\nThis ensures that the model is trained only on past observations and evaluated on genuinely later time periods.\nAfter splitting, we compute feature-wise means and standard deviations only on the training set. These statistics are collected for each model input feature in feature_cols. They are later used to apply z-score normalization to both training and test sequences. Computing normalization parameters solely from the training set avoids information leakage from the future into the model.\nThe resulting arrays mean and std store the normalization parameters in a consistent order that matches the feature layout used by NumPy and PyTorch.\n\n# 80% threshold\nimport numpy as np\n\nthreshold = con.execute(\"\"\"\n    SELECT quantile(time_index, 0.8) FROM flow_feat;\n\"\"\").fetchone()[0]\n\ncon.execute(\"\"\"\n    CREATE OR REPLACE TABLE flow_feat_train AS\n    SELECT * FROM flow_feat\n    WHERE time_index &lt;= ?;\n\"\"\", [threshold])\n\ncon.execute(\"\"\"\n    CREATE OR REPLACE TABLE flow_feat_test AS\n    SELECT * FROM flow_feat\n    WHERE time_index &gt; ?;\n\"\"\", [threshold])\n\n# mean / std\nmean_row = con.execute(f\"\"\"\n    SELECT {\", \".join([f\"avg({c}) AS {c}\" for c in feature_cols])}\n    FROM flow_feat_train;\n\"\"\").fetchnumpy()\n\nstd_row = con.execute(f\"\"\"\n    SELECT {\", \".join([f\"stddev_samp({c}) AS {c}\" for c in feature_cols])}\n    FROM flow_feat_train;\n\"\"\").fetchnumpy()\n\nmean = np.array([mean_row[c][0] for c in feature_cols], dtype=\"float32\")\nstd  = np.array([std_row[c][0]  for c in feature_cols], dtype=\"float32\") + 1e-6\n\n\n\n3. Sequence construction\nTo train temporal models, we convert the flat table flow_feat into short, fixed-length sequences for each grid cell. These sequences represent a brief history of recent observations, followed by the flow value that occurs immediately afterward.\nThe construction process works cell by cell. For each table (flow_feat_train or flow_feat_test), we first identify every grid cell that contains at least SEQ_LEN time steps. For each eligible cell, we retrieve all of its records ordered by the global time_index so that the time series is consistent and gap-free.\nFrom this ordered sequence, we slide a window of length SEQ_LEN over the cell’s history. The window moves forward by a fixed stride (e.g., 4 time steps) to avoid generating overly similar samples. Each window contains:\n\na sequence of SEQ_LEN standardized feature vectors, representing the cell’s recent historical states;\na target value equal to the flow_next associated with the final time step inside that window.\n\nIn other words, every training example describes “the previous eight half-hours” and pairs it with “the flow in the next half-hour.” The generator bundles these examples into mini-batches:\n\nX_batch has shape [batch_size, SEQ_LEN, feature_dim], containing the input histories;\ny_batch has shape [batch_size], containing the corresponding next-step flow targets.\n\nThese mini-batches form the direct input to both the linear model and the LSTM model during training and evaluation.\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nSEQ_LEN   = 8\nBATCH_SIZE = 1024\nSTRIDE    = 4\n\ndef seq_batch_generator(table_name, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, max_cells=None):\n    \"\"\"\n    Generate sequence batches from flow_feat_train / flow_feat_test.\n\n    X_batch: [batch, seq_len, feature_dim]\n    y_batch: [batch]\n    \"\"\"\n    cells = con.execute(f\"\"\"\n        SELECT x, y\n        FROM {table_name}\n        GROUP BY x, y\n        HAVING COUNT(*) &gt;= {seq_len}\n    \"\"\").fetchall()\n\n    if max_cells is not None:\n        cells = cells[:max_cells]\n\n    print(f\"{table_name}: {len(cells)} cells with &gt;= {seq_len} records\")\n\n    for (x_val, y_val) in cells:\n        res = con.execute(f\"\"\"\n            SELECT time_index, {\", \".join(feature_cols)}, flow_next\n            FROM {table_name}\n            WHERE x = ? AND y = ?\n            ORDER BY time_index\n        \"\"\", [x_val, y_val]).fetchnumpy()\n\n        n = len(res[\"flow_next\"])\n        if n &lt; seq_len:\n            continue\n\n        feats = np.column_stack([res[c] for c in feature_cols]).astype(\"float32\")\n        feats = (feats - mean) / std\n\n        y_vec = res[\"flow_next\"].astype(\"float32\")\n\n        X_buf, y_buf = [], []\n\n        for i in range(0, n - seq_len + 1, STRIDE):\n            X_seq = feats[i:i+seq_len]           # [seq_len, F]\n            y_target = y_vec[i + seq_len - 1]    # scalar\n\n            X_buf.append(X_seq)\n            y_buf.append(y_target)\n\n            if len(X_buf) == batch_size:\n                yield np.stack(X_buf), np.array(y_buf)\n                X_buf, y_buf = [], []\n\n        if len(X_buf) &gt; 0:\n            yield np.stack(X_buf), np.array(y_buf)\n\n\n\n\n4. Models\n\n4.1 LSTM model\nThe first predictive model we use is an LSTM-based sequence model designed to capture temporal structure in mobility flows. Each input sample is a fixed-length sequence containing SEQ_LEN consecutive half-hour records for a single grid cell. Each record has the full feature vector consisting of the flow value, POI-based attributes, and the time-encoding variables.\nThe LSTM processes these sequences one time step at a time. As it moves through the eight input steps, it maintains a hidden state that summarizes the information accumulated so far. After the final time step, the hidden state represents the model’s understanding of the recent history of the cell.\nTo make a prediction, we take this final hidden state and feed it into a fully connected layer that outputs a single value: the predicted next-step flow. This structure allows the model to learn temporal dependencies, non-linear feature interactions, and recurring movement patterns across days.\nThe LSTM used here has: - batch-first input format, - a hidden size of 64, - two stacked recurrent layers, - and a final linear layer that maps the hidden representation into a scalar prediction.\nThe model is trained end-to-end with the Adam optimizer and mean squared error as the loss function.\n\nimport torch\nfrom torch import nn\n\ninput_dim = len(feature_cols)\nhidden_dim = 64\nnum_layers = 2\n\nclass FlowLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True\n        )\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        # x: [B, L, F]\n        out, _ = self.lstm(x)        \n        last_hidden = out[:, -1, :]   \n        y_pred = self.fc(last_hidden)  \n        return y_pred.squeeze(-1)     \n\n\nclass LinearSeqModel(nn.Module):\n    def __init__(self, input_dim, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.fc = nn.Linear(input_dim * seq_len, 1)\n\n    def forward(self, x):\n        # x: [B, L, F]\n        B, L, F = x.shape\n        x_flat = x.reshape(B, L * F)  \n        y_pred = self.fc(x_flat)       \n        return y_pred.squeeze(-1)     \n\n\n\n4.2 Linear sequence baseline\nTo provide a simple point of comparison, we also include a linear baseline model. Instead of using a recurrent structure, this model treats the entire input sequence as a single flat vector. All SEQ_LEN feature rows are concatenated into one long input, and the model applies a single fully connected layer to map this vector directly to a predicted next-step flow.\nThis baseline has no notion of temporal order or temporal dependency. It can only learn a weighted combination of the raw features but cannot model how patterns evolve over time. As a result, it serves as a useful reference for evaluating whether the LSTM’s recurrent structure truly provides additional predictive power.\nDespite its simplicity, the linear model is fast to train and helps quantify the value of incorporating temporal dynamics into the prediction task.\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\ninput_dim = len(feature_cols)\nhidden_dim = 64\nnum_layers = 2\n\nclass FlowLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True\n        )\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        # x: [B, L, F]\n        out, _ = self.lstm(x)          # out: [B, L, H]\n        last_hidden = out[:, -1, :]    # [B, H] -&gt; last time step\n        y_pred = self.fc(last_hidden)  # [B, 1]\n        return y_pred.squeeze(-1)      # [B]\n\nmodel = FlowLSTM(input_dim, hidden_dim, num_layers).to(device)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nprint(model)\n\n\n@torch.no_grad()\ndef evaluate_mse_seq(table_name: str) -&gt; float:\n    model.eval()\n    sq_sum = 0.0\n    n = 0\n    for X_batch, y_batch in seq_batch_generator(table_name, batch_size=BATCH_SIZE, seq_len=SEQ_LEN):\n        xb = torch.from_numpy(X_batch).to(device)  # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)  # [B]\n        pred = model(xb)                           # [B]\n        sq_sum += ((pred - yb) ** 2).sum().item()\n        n += len(yb)\n    return sq_sum / n\n\n\n\n\n5. Training\nBoth the LSTM model and the linear baseline model are trained using the mini-batches produced by the sequence generator. Each batch contains a set of short historical sequences for different grid cells, paired with the flow value that occurs immediately afterward.\nDuring training, the model loops over all batches from flow_feat_train. For each batch:\n\nThe input sequences are passed through the model to obtain predictions of the next-step flow.\nThese predictions are compared with the true targets using mean squared error as the loss function.\nThe optimizer (Adam) updates the model parameters to reduce this loss.\n\nThis process is repeated for several epochs. After each epoch, the model is evaluated on flow_feat_test using the same sequence construction procedure. The test-set mean squared error provides a forward-looking measure of performance, since the test rows correspond to later time periods that were not used during training.\nFor the LSTM model, training checkpoints are saved after each epoch, including the model parameters, optimizer state, and the current training and test metrics. A final set of model weights is also saved separately for inference and visualization.\n\nimport torch\nfrom torch import nn\n\nclass LinearSeqModel(nn.Module):\n    def __init__(self, input_dim: int, seq_len: int):\n        \"\"\"\n        input_dim: len(feature_cols)\n        seq_len  : SEQ_LEN\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.seq_len = seq_len\n        self.fc = nn.Linear(input_dim * seq_len, 1)\n\n    def forward(self, x):\n        \"\"\"\n        x: [B, L*F]\n        \"\"\"\n        y_pred = self.fc(x)          # [B, 1]\n        return y_pred.squeeze(-1)    # [B]\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\ninput_dim = len(feature_cols)\n\nlin_model = LinearSeqModel(input_dim=input_dim, seq_len=SEQ_LEN).to(device)\nlin_loss_fn = nn.MSELoss()\nlin_optimizer = torch.optim.Adam(lin_model.parameters(), lr=1e-3)\n\nprint(lin_model)\n\n\n@torch.no_grad()\ndef evaluate_mse_linear(model, table_name: str, max_cells=None) -&gt; float:\n    model.eval()\n    sq_sum = 0.0\n    n = 0\n\n    for X_batch, y_batch in seq_batch_generator(\n        table_name,\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=max_cells\n    ):\n        xb = torch.from_numpy(X_batch).to(device)    # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)    # [B]\n\n        xb = xb.reshape(xb.shape[0], -1)             # [B, L*F]\n\n        pred = model(xb)                             # [B]\n        sq_sum += ((pred - yb) ** 2).sum().item()\n        n += len(yb)\n\n    return sq_sum / n\n\n\nEPOCHS_LIN = 8\n\nfor epoch in range(1, EPOCHS_LIN + 1):\n    lin_model.train()\n    train_sq_sum = 0.0\n    train_n = 0\n    batch_idx = 0\n\n    for X_batch, y_batch in seq_batch_generator(\n        \"flow_feat_train\",\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=None\n    ):\n        xb = torch.from_numpy(X_batch).to(device)    # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)    # [B]\n\n        # flatten [B, L*F]\n        xb = xb.reshape(xb.shape[0], -1)\n\n        lin_optimizer.zero_grad()\n        pred = lin_model(xb)                         # [B]\n        loss = lin_loss_fn(pred, yb)\n        loss.backward()\n        lin_optimizer.step()\n\n        train_sq_sum += ((pred.detach() - yb) ** 2).sum().item()\n        train_n += len(yb)\n\n        batch_idx += 1\n        if batch_idx % 100 == 0:\n            print(f\"[Linear] Epoch {epoch:02d} | Processed {batch_idx} batches\")\n\n    #  train / test MSE\n    train_mse = train_sq_sum / train_n\n    test_mse = evaluate_mse_linear(lin_model, \"flow_feat_test\", max_cells=None)\n\n    print(f\"[Linear] Epoch {epoch:02d} | Train MSE {train_mse:.4f} | Test MSE {test_mse:.4f}\")\n\ntorch.save(\n    lin_model.state_dict(),\n    \"/content/drive/MyDrive/MOBILITY/flow_linear_seq_epoch8.pth\"\n)\nprint(\"Saved linear baseline model.\")\n\n\n\nimport os\n\n# checkpoint\nckpt_dir = \"/content/drive/MyDrive/MOBILITY/checkpoints_lstm\"\nos.makedirs(ckpt_dir, exist_ok=True)\n\n# ----- training loop ----- checkpoint\nEPOCHS = 8\n\n@torch.no_grad()\ndef evaluate_mse_seq(table_name: str, max_cells=None) -&gt; float:\n    model.eval()\n    sq_sum = 0.0\n    n = 0\n    for X_batch, y_batch in seq_batch_generator(\n        table_name,\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=max_cells\n    ):\n        xb = torch.from_numpy(X_batch).to(device)  # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)  # [B]\n        pred = model(xb)                           # [B]\n        sq_sum += ((pred - yb) ** 2).sum().item()\n        n += len(yb)\n    return sq_sum / n\n\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    train_sq_sum = 0.0\n    train_n = 0\n\n    batch_idx = 0\n\n    # Here you can set max_cells for debugging; set to None for full data\n    for X_batch, y_batch in seq_batch_generator(\n        \"flow_feat_train\",\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=None\n    ):\n        xb = torch.from_numpy(X_batch).to(device)\n        yb = torch.from_numpy(y_batch).to(device)\n\n        optimizer.zero_grad()\n        pred = model(xb)\n        loss = loss_fn(pred, yb)\n        loss.backward()\n        optimizer.step()\n\n        train_sq_sum += ((pred.detach() - yb) ** 2).sum().item()\n        train_n += len(yb)\n\n        batch_idx += 1\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch:02d} | Processed {batch_idx} batches so far\")\n\n    train_mse = train_sq_sum / train_n\n    test_mse = evaluate_mse_seq(\"flow_feat_test\", max_cells=None)\n\n    print(f\"Epoch {epoch:02d} | Train MSE {train_mse:.4f} | Test MSE {test_mse:.4f}\")\n\n    ckpt_path = os.path.join(ckpt_dir, f\"flow_lstm_epoch{epoch:02d}.pth\")\n    torch.save(\n        {\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"train_mse\": train_mse,\n            \"test_mse\": test_mse,\n        },\n        ckpt_path,\n    )\n    print(f\"Saved checkpoint to {ckpt_path}\")\n\nfinal_path = \"/content/drive/MyDrive/MOBILITY/flow_lstm_final.pth\"\ntorch.save(model.state_dict(), final_path)\nprint(f\"Saved final model to {final_path}\")\n\n\n\n\n6. Spatial visualization\nTo understand where and when the models perform well, we visualize predictions across the spatial grid. The idea is to fix a specific day and one or more time-of-day slots, reconstruct the historical sequences for each grid cell at those times, run the trained model, and compare its predictions with the true flows.\nThe visualization works in two modes:\n\nSingle time slice\n\nChoose a day and an eight-step window within that day.\nFor the final time step of the window, identify all grid cells that have a valid next-step flow.\nFor each of these cells, retrieve the preceding SEQ_LEN records to form a complete history.\nAfter running the model, create a spatial plot showing:\n\nthe true next-step flow,\nthe model’s predicted flow,\nand the absolute error.\n\n\n\n\nd0 = 64\n\nimport matplotlib.pyplot as plt\n\nstart_t = 16\nwindow_len = 8\nend_t = start_t + window_len - 1\n\nwindow_df = con.execute(\"\"\"\n    SELECT\n        x,\n        y,\n        (time_index % 48) AS t,\n        flow\n    FROM flow_feat_test\n    WHERE (time_index / 48)::INT = ?\n      AND (time_index % 48) BETWEEN ? AND ?\n\"\"\", [d0, start_t, end_t]).fetchdf()\n\nprint(window_df.head())\nprint(\"records have =\", len(window_df))\n\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\nt_plot = end_t\n\nsnap_true = con.execute(f\"\"\"\n    SELECT\n        x,\n        y,\n        flow_next AS target,\n        time_index\n    FROM flow_feat_test\n    WHERE (time_index / 48)::INT = ?\n      AND (time_index % 48) = ?;\n\"\"\", [d0, t_plot]).fetchdf()\n\nprint(\"cells at this time step:\", len(snap_true))\n\n\n\nX_list, target_list, x_list, y_list = [], [], [], []\n\nfor idx, row in snap_true.iterrows():\n    x0 = int(row[\"x\"])\n    y0 = int(row[\"y\"])\n    T  = int(row[\"time_index\"])\n    y_true = float(row[\"target\"])\n\n    seq_df = con.execute(f\"\"\"\n        SELECT time_index, {\", \".join(feature_cols)}\n        FROM (\n            SELECT time_index, {\", \".join(feature_cols)}\n            FROM flow_feat_test\n            WHERE x = ? AND y = ?\n              AND (time_index / 48)::INT = ?\n              AND time_index &lt;= ?\n            ORDER BY time_index DESC\n            LIMIT ?\n        )\n        ORDER BY time_index;\n    \"\"\", [x0, y0, d0, T, SEQ_LEN]).fetchdf()\n\n    if len(seq_df) &lt; SEQ_LEN:\n        continue\n\n    feats = seq_df[feature_cols].to_numpy(dtype=\"float32\")\n    feats_norm = (feats - mean) / std\n\n    X_list.append(feats_norm)\n    target_list.append(y_true)\n    x_list.append(x0)\n    y_list.append(y0)\n\nprint(\"usable cells with full 8-step history:\", len(X_list))\n\nX = np.stack(X_list).astype(\"float32\")       # [N, L, F]\ny_true_arr = np.array(target_list, dtype=\"float32\")\n\nmodel.eval()\nwith torch.no_grad():\n    xb = torch.from_numpy(X).to(device)     # [N, L, F]\n    pred_arr = model(xb).cpu().numpy()      # [N]\n\n\nsnap = pd.DataFrame({\n    \"x\": x_list,\n    \"y\": y_list,\n    \"flow_true\": y_true_arr,     # true next-step flow\n    \"flow_pred\": pred_arr,       # LSTM predicted next-step flow\n})\nsnap[\"err_abs\"] = np.abs(snap[\"flow_pred\"] - snap[\"flow_true\"])\n\n\nimport matplotlib.pyplot as plt\n\nvmin = np.percentile(snap[\"flow_true\"], 5)\nvmax = np.percentile(snap[\"flow_true\"], 95)\n\nerr_max = np.percentile(snap[\"err_abs\"], 95)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n\nax = axes[0]\nsc0 = ax.scatter(\n    snap[\"x\"], snap[\"y\"],\n    c=snap[\"flow_true\"],\n    s=10,\n    cmap=\"plasma\",\n    vmin=vmin,\n    vmax=vmax\n)\nax.set_title(\"True next-step flow\")\nax.set_aspect(\"equal\")\nax.grid(False)\nplt.colorbar(sc0, ax=ax, label=\"flow_true\")\n\nax = axes[1]\nsc1 = ax.scatter(\n    snap[\"x\"], snap[\"y\"],\n    c=snap[\"flow_pred\"],\n    s=10,\n    cmap=\"plasma\",\n    vmin=vmin,\n    vmax=vmax\n)\nax.set_title(\"LSTM predicted next-step flow\")\nax.set_aspect(\"equal\")\nax.grid(False)\nplt.colorbar(sc1, ax=ax, label=\"flow_pred\")\n\nax = axes[2]\nsc2 = ax.scatter(\n    snap[\"x\"], snap[\"y\"],\n    c=snap[\"err_abs\"],\n    s=10,\n    cmap=\"magma\",\n    vmin=0,\n    vmax=err_max\n)\nax.set_title(\"|pred − true|\")\nax.set_aspect(\"equal\")\nax.grid(False)\nplt.colorbar(sc2, ax=ax, label=\"abs error\")\n\nfor ax in axes:\n    ax.set_xlabel(\"x\")\naxes[0].set_ylabel(\"y\")\n\nminutes = t_plot * 30\nhh = minutes // 60\nmm = minutes % 60\nplt.suptitle(f\"Day {d0}, t={t_plot} (~{hh:02d}:{mm:02d}) – next-step flow\", fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\n\n\nMultiple time slices\n\nSelect several time-of-day indices on the same day.\nRepeat the above process for each time, stacking the resulting maps side-by-side.\nThe first row displays true flows, the second row shows predictions, and the third row shows absolute errors.\nShared color scales allow visual comparison across time.\n\n\nThese visualizations reveal spatial heterogeneity in both mobility patterns and model performance. They help identify when the model struggles, which regions tend to be more predictable, and how flow dynamics evolve over the day.\n\nd0 = 64\n\nt_list = [8, 12, 16, 20, 24, 28]\n\nsnap_list = []\n\nmodel.eval()\n\nfor t_plot in t_list:\n    snap_true = con.execute(\"\"\"\n        SELECT\n            x,\n            y,\n            flow_next AS target,\n            time_index\n        FROM flow_feat_test\n        WHERE (time_index / 48)::INT = ?\n          AND (time_index % 48) = ?;\n    \"\"\", [d0, t_plot]).fetchdf()\n\n    X_list, target_list, x_list, y_list = [], [], [], []\n\n    for idx, row in snap_true.iterrows():\n        x0 = int(row[\"x\"])\n        y0 = int(row[\"y\"])\n        T  = int(row[\"time_index\"])\n        y_true = float(row[\"target\"])\n\n        seq_df = con.execute(f\"\"\"\n            SELECT time_index, {\", \".join(feature_cols)}\n            FROM (\n                SELECT time_index, {\", \".join(feature_cols)}\n                FROM flow_feat_test\n                WHERE x = ? AND y = ?\n                  AND (time_index / 48)::INT = ?\n                  AND time_index &lt;= ?\n                ORDER BY time_index DESC\n                LIMIT ?\n            )\n            ORDER BY time_index;\n        \"\"\", [x0, y0, d0, T, SEQ_LEN]).fetchdf()\n\n        if len(seq_df) &lt; SEQ_LEN:\n            continue\n\n        feats = seq_df[feature_cols].to_numpy(dtype=\"float32\")\n        feats_norm = (feats - mean) / std\n\n        X_list.append(feats_norm)\n        target_list.append(y_true)\n        x_list.append(x0)\n        y_list.append(y0)\n\n    if len(X_list) == 0:\n        print(f\"t={t_plot} no enough\")\n        continue\n\n    X = np.stack(X_list).astype(\"float32\")\n    y_true_arr = np.array(target_list, dtype=\"float32\")\n\n    with torch.no_grad():\n        xb = torch.from_numpy(X).to(device)\n        y_pred_arr = model(xb).cpu().numpy()\n\n    snap = pd.DataFrame({\n        \"x\": x_list,\n        \"y\": y_list,\n        \"flow_true\": y_true_arr,\n        \"flow_pred\": y_pred_arr,\n    })\n    snap[\"err_abs\"] = np.abs(snap[\"flow_pred\"] - snap[\"flow_true\"])\n    snap[\"t_plot\"] = t_plot\n\n    snap_list.append(snap)\n\nprint(\"usable time points\", len(snap_list))\n\n\nall_snap = pd.concat(snap_list, ignore_index=True)\nvmin = np.percentile(all_snap[\"flow_true\"], 5)\nvmax = np.percentile(all_snap[\"flow_true\"], 95)\nerr_max = np.percentile(all_snap[\"err_abs\"], 95)\n\nn_times = len(snap_list)\nsnap_list_sorted = sorted(snap_list, key=lambda s: s[\"t_plot\"].iloc[0])\n\nfig, axes = plt.subplots(\n    3, n_times,\n    figsize=(3.0 * n_times, 8),\n    sharex=True, sharey=True\n)\n\nsc_true = None\nsc_err = None\n\nfor j, snap in enumerate(snap_list_sorted):\n    t_plot = snap[\"t_plot\"].iloc[0]\n\n    ax = axes[0, j]\n    sc_true = ax.scatter(\n        snap[\"x\"], snap[\"y\"],\n        c=snap[\"flow_true\"],\n        s=5,\n        cmap=\"plasma\",\n        vmin=vmin, vmax=vmax\n    )\n    ax.set_title(f\"t={t_plot} True\", fontsize=10)\n    ax.set_aspect(\"equal\")\n    ax.grid(False)\n\n    ax = axes[1, j]\n    ax.scatter(\n        snap[\"x\"], snap[\"y\"],\n        c=snap[\"flow_pred\"],\n        s=5,\n        cmap=\"plasma\",\n        vmin=vmin, vmax=vmax\n    )\n    ax.set_title(f\"t={t_plot} Pred\", fontsize=10)\n    ax.set_aspect(\"equal\")\n    ax.grid(False)\n\n    ax = axes[2, j]\n    sc_err = ax.scatter(\n        snap[\"x\"], snap[\"y\"],\n        c=snap[\"err_abs\"],\n        s=5,\n        cmap=\"magma\",\n        vmin=0, vmax=err_max\n    )\n    ax.set_title(f\"t={t_plot} |Pred−True|\", fontsize=10)\n    ax.set_aspect(\"equal\")\n    ax.grid(False)\n\nfor j in range(n_times):\n    axes[2, j].set_xlabel(\"x\")\nfor i in range(3):\n    axes[i, 0].set_ylabel(\"y\")\n\nplt.subplots_adjust(\n    left=0.06, right=0.9,\n    top=0.9, bottom=0.08,\n    wspace=0.05, hspace=0.12\n)\n\ncbar1 = fig.colorbar(\n    sc_true,\n    ax=axes[0:2, -1],\n    fraction=0.046,\n    pad=0.02\n)\ncbar1.set_label(\"flow\")\n\ncbar2 = fig.colorbar(\n    sc_err,\n    ax=axes[2, -1],\n    fraction=0.046,\n    pad=0.04\n)\ncbar2.set_label(\"abs error\")\n\nplt.suptitle(f\"Day {d0} – 6 time steps: True / Pred / Error\", fontsize=14)\nplt.show()\n\n\n\n\nConclusion\nThe results demonstrate that temporal modeling is essential for predicting grid-based mobility flows. The linear baseline, which treats the input sequence as a single flattened vector, fails to capture temporal structure: its test MSE is more than two orders of magnitude worse than that of the LSTM. In contrast, the LSTM achieves stable performance (train MSE ≈ 3.79, test MSE ≈ 4.15), indicating that it successfully learns short-term temporal dependencies in the flow dynamics.\nSpatial visualizations further confirm this pattern. Across six representative time steps on Day 64, the LSTM reproduces the broad spatial structure of flow, accurately matching both low-flow regions and the major concentration zones. Prediction errors are primarily concentrated in high-intensity areas—locations where flow is both larger in magnitude and more volatile. This suggests that the LSTM captures routine, low-variance mobility well, while sudden spikes or irregular flow patterns remain harder to model.\nOverall, the experiment shows that: 1. Temporal information is crucial for mobility forecasting. 2. Even a relatively small LSTM (hidden size 64, two layers) is sufficient to outperform non-temporal baselines by a wide margin. 3. The remaining errors are spatially structured, pointing toward opportunities for future extensions such as attention mechanisms or spatial graph models.\nThe LSTM therefore provides a strong foundation for next-step mobility prediction, and the spatial evaluation highlights where more advanced models may yield additional gains."
  }
]