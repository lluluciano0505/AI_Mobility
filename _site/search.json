[
  {
    "objectID": "model222_ipyn.html",
    "href": "model222_ipyn.html",
    "title": "LSTM Spatial Flow Prediction: True Values, Predictions, and Absolute Error",
    "section": "",
    "text": "1. Data and feature construction\nWe start from a DuckDB database mobility.duckdb stored in Google Drive.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport duckdb\nimport pandas as pd\n\ncon = duckdb.connect(\"/content/drive/MyDrive/MOBILITY/mobility.duckdb\")\n\ncon.execute(\"SHOW TABLES;\").fetchdf()\n\n\n1.1 Inspecting the base table\nI first examine the DuckDB database to understand the structure of the input data. All available tables are listed to confirm that the base table flow_poi is present, and the schema of flow_poi is queried to obtain the full set of column names.\nFrom these columns, all fields whose names start with poi_ are selected automatically. These poi_* variables describe the point-of-interest (POI) environment of each grid cell—for example, counts of food, shopping, or entertainment locations nearby. Together they form a POI feature vector, where each element represents the intensity of a POI category and provides a compact numerical description of local urban context.\nA consistent feature ordering is then defined for later use in NumPy and PyTorch. The feature list begins with the current flow value, followed by all poi_* columns, and ends with four time-encoding features added in the next step: tod_sin, tod_cos, dow_sin, and dow_cos.\n\ninfo = con.execute(\"PRAGMA table_info('flow_poi');\").fetchall()\nall_cols = [r[1] for r in info]\npoi_cols = [c for c in all_cols if c.startswith(\"poi_\")]\n\nprint(\"lens of POI\", len(poi_cols))\nprint(\"5 poi:\", poi_cols[:5])\n\n\n\n\n1.2 Building the modeling table flow_feat\nNext, a cleaned and feature-rich modeling table, flow_feat, is constructed directly inside DuckDB. This table transforms the raw records in flow_poi into supervised learning examples suitable for sequence models.\nConstruction proceeds in two steps. In the first step:\n\nthe day (d), time (t), and grid indices (x, y) are cast to integers;\nmissing values in all poi_* columns are replaced with 0, ensuring POI features are defined for every cell;\ntime_of_day = t is defined as a half-hour index from 0 to 47;\ndow = d % 7 is defined as a day-of-week index from 0 to 6;\na global time_index = d * 48 + t is created to flatten (day, half-hour) into a single time axis.\n\nIn the second step, each row is enriched with temporal encodings and a prediction target:\n\nsinusoidal encodings of time-of-day (tod_sin, tod_cos) are computed to represent the 48 half-hour slots within a day on a circular scale;\nsinusoidal encodings of day-of-week (dow_sin, dow_cos) are computed to represent weekly periodicity;\nflow_next is defined as the next half-hour flow for the same grid cell, using a window function over each (x, y) time series ordered by (d, t).\n\nFinally, the grid indices (x, y), all model input features (flow, all poi_* columns, and the four time encodings), the target flow_next, and the global time_index are selected. Rows with missing flow_next (i.e., the final time step in a cell’s sequence) are dropped. The resulting flow_feat table is a clean, fully specified dataset ready for temporal train/test splitting and sequence construction.\n\nfeature_cols = [\"flow\"] + poi_cols + [\"tod_sin\", \"tod_cos\", \"dow_sin\", \"dow_cos\"]\n\ncon.execute(f\"\"\"\n    CREATE OR REPLACE TABLE flow_feat AS\n    WITH base AS (\n        SELECT\n            d::INT AS d,\n            t::INT AS t,\n            x::INT AS x,\n            y::INT AS y,\n            flow,\n            {\", \".join([f\"COALESCE({c}, 0) AS {c}\" for c in poi_cols])},\n            t AS time_of_day,\n            d % 7 AS dow,\n            (d * 48 + t) AS time_index\n        FROM flow_poi\n    ),\n    feat AS (\n        SELECT\n            *,\n            sin(2*PI()*time_of_day/48.0) AS tod_sin,\n            cos(2*PI()*time_of_day/48.0) AS tod_cos,\n            sin(2*PI()*dow/7.0)          AS dow_sin,\n            cos(2*PI()*dow/7.0)          AS dow_cos,\n            LEAD(flow) OVER (\n                PARTITION BY x, y\n                ORDER BY d, t\n            ) AS flow_next\n        FROM base\n    )\n    SELECT\n        x,\n        y,\n        {\", \".join(feature_cols)},\n        flow_next,\n        time_index\n    FROM feat\n    WHERE flow_next IS NOT NULL;\n\"\"\")\n\n\nprint(con.execute(\"SELECT COUNT(*) FROM flow_feat;\").fetchall())\nprint(con.execute(\"PRAGMA table_info('flow_feat');\").fetchall()[:15])\n\n\n\n\n\n2. Train–test split and feature normalization\nTo prepare the data for modeling, a temporal split is applied to the feature table flow_feat. Because mobility patterns evolve over time, evaluation should be performed on future periods rather than on randomly shuffled samples. The dataset is therefore split using the global time_index:\n\nthe 80th percentile of time_index is computed across all rows;\nrows with time_index ≤ threshold form the training set (flow_feat_train);\nrows with time_index &gt; threshold form the test set (flow_feat_test).\n\nThis split trains the model on earlier observations and evaluates it on genuinely later time periods.\nAfter splitting, feature-wise means and standard deviations are computed only from the training set. These statistics are calculated for each model input feature in feature_cols and are later used to apply z-score normalization to both training and test sequences. Estimating normalization parameters from the training portion only avoids leaking information from the future into the model.\nThe resulting arrays, mean and std, store normalization parameters in a fixed order consistent with the feature layout used by NumPy and PyTorch.\n\n# 80% threshold\nimport numpy as np\n\nthreshold = con.execute(\"\"\"\n    SELECT quantile(time_index, 0.8) FROM flow_feat;\n\"\"\").fetchone()[0]\n\ncon.execute(\"\"\"\n    CREATE OR REPLACE TABLE flow_feat_train AS\n    SELECT * FROM flow_feat\n    WHERE time_index &lt;= ?;\n\"\"\", [threshold])\n\ncon.execute(\"\"\"\n    CREATE OR REPLACE TABLE flow_feat_test AS\n    SELECT * FROM flow_feat\n    WHERE time_index &gt; ?;\n\"\"\", [threshold])\n\n# mean / std\nmean_row = con.execute(f\"\"\"\n    SELECT {\", \".join([f\"avg({c}) AS {c}\" for c in feature_cols])}\n    FROM flow_feat_train;\n\"\"\").fetchnumpy()\n\nstd_row = con.execute(f\"\"\"\n    SELECT {\", \".join([f\"stddev_samp({c}) AS {c}\" for c in feature_cols])}\n    FROM flow_feat_train;\n\"\"\").fetchnumpy()\n\nmean = np.array([mean_row[c][0] for c in feature_cols], dtype=\"float32\")\nstd  = np.array([std_row[c][0]  for c in feature_cols], dtype=\"float32\") + 1e-6\n\n\n\n3. Sequence Construction\nTraining sequences are constructed using three hyperparameters: SEQ_LEN = 8, STRIDE = 4, and BATCH_SIZE = 1024. With SEQ_LEN = 8, each sample consists of the most recent eight half-hour observations for a given grid cell, corresponding to a 4-hour historical window. A STRIDE of 4 shifts the sliding window forward by four time steps (two hours), preventing consecutive samples from being nearly identical. BATCH_SIZE = 1024 determines how many sequences are processed together in each mini-batch, providing a balance between computational efficiency and training stability.\nSequence generation begins from the tables flow_feat_train and flow_feat_test, with sequences built independently for each grid cell (x,y). Only cells appearing at least eight times are retained, ensuring that a full sequence of length SEQ_LEN is available. For each eligible cell, its rows are sorted by the global time_index to form an ordered time series. The columns listed in feature_cols are then extracted and standardized using the mean and standard deviation from the training set, placing all features on a comparable scale.\nA fixed-length sliding window of size 8 is applied to each standardized cell-level time series, with window start positions at i=0,4,8,…, reflecting the chosen stride. Each window produces an input sequence consisting of eight consecutive half-hour records, and the prediction target is defined as the flow_next value associated with the final row in the window. Conceptually, each training example captures a 4-hour history and asks the model to predict the flow in the subsequent half-hour.\nAll resulting examples are grouped into mini-batches of size 1024. Each batch tensor X_batch has shape [1024,8,feature_dim], containing 1024 standardized sequences of length 8. The corresponding target vector y_batch has shape [1024], holding the next-step flow values. Both the linear baseline model and the LSTM model consume the same batch format during training and evaluation, meaning that any performance differences arise solely from how the two architectures interpret the same sequence input.\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nSEQ_LEN   = 8\nBATCH_SIZE = 1024\nSTRIDE    = 4\n\ndef seq_batch_generator(table_name, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, max_cells=None):\n    \"\"\"\n    Generate sequence batches from flow_feat_train / flow_feat_test.\n\n    X_batch: [batch, seq_len, feature_dim]\n    y_batch: [batch]\n    \"\"\"\n    cells = con.execute(f\"\"\"\n        SELECT x, y\n        FROM {table_name}\n        GROUP BY x, y\n        HAVING COUNT(*) &gt;= {seq_len}\n    \"\"\").fetchall()\n\n    if max_cells is not None:\n        cells = cells[:max_cells]\n\n    print(f\"{table_name}: {len(cells)} cells with &gt;= {seq_len} records\")\n\n    for (x_val, y_val) in cells:\n        res = con.execute(f\"\"\"\n            SELECT time_index, {\", \".join(feature_cols)}, flow_next\n            FROM {table_name}\n            WHERE x = ? AND y = ?\n            ORDER BY time_index\n        \"\"\", [x_val, y_val]).fetchnumpy()\n\n        n = len(res[\"flow_next\"])\n        if n &lt; seq_len:\n            continue\n\n        feats = np.column_stack([res[c] for c in feature_cols]).astype(\"float32\")\n        feats = (feats - mean) / std\n\n        y_vec = res[\"flow_next\"].astype(\"float32\")\n\n        X_buf, y_buf = [], []\n\n        for i in range(0, n - seq_len + 1, STRIDE):\n            X_seq = feats[i:i+seq_len]           # [seq_len, F]\n            y_target = y_vec[i + seq_len - 1]    # scalar\n\n            X_buf.append(X_seq)\n            y_buf.append(y_target)\n\n            if len(X_buf) == batch_size:\n                yield np.stack(X_buf), np.array(y_buf)\n                X_buf, y_buf = [], []\n\n        if len(X_buf) &gt; 0:\n            yield np.stack(X_buf), np.array(y_buf)\n\n\n\n\n4. Models\n\n4.1 LSTM model\nThe first predictive model we use is an LSTM-based sequence model designed to capture temporal structure in mobility flows. Each input sample is a fixed-length sequence containing SEQ_LEN consecutive half-hour records for a single grid cell. Each record has the full feature vector consisting of the flow value, POI-based attributes, and the time-encoding variables.\nThe LSTM processes these sequences one time step at a time. As it moves through the eight input steps, it maintains a hidden state that summarizes the information accumulated so far. After the final time step, the hidden state represents the model’s understanding of the recent history of the cell.\nTo make a prediction, we take this final hidden state and feed it into a fully connected layer that outputs a single value: the predicted next-step flow. This structure allows the model to learn temporal dependencies, non-linear feature interactions, and recurring movement patterns across days.\nThe LSTM used here has: - batch-first input format, - a hidden size of 64, - two stacked recurrent layers, - and a final linear layer that maps the hidden representation into a scalar prediction.\nThe model is trained end-to-end with the Adam optimizer and mean squared error as the loss function.\n\nimport torch\nfrom torch import nn\n\ninput_dim = len(feature_cols)\nhidden_dim = 64\nnum_layers = 2\n\nclass FlowLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True\n        )\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        # x: [B, L, F]\n        out, _ = self.lstm(x)        \n        last_hidden = out[:, -1, :]   \n        y_pred = self.fc(last_hidden)  \n        return y_pred.squeeze(-1)     \n\n\nclass LinearSeqModel(nn.Module):\n    def __init__(self, input_dim, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.fc = nn.Linear(input_dim * seq_len, 1)\n\n    def forward(self, x):\n        # x: [B, L, F]\n        B, L, F = x.shape\n        x_flat = x.reshape(B, L * F)  \n        y_pred = self.fc(x_flat)       \n        return y_pred.squeeze(-1)     \n\n\n\n4.2 Linear sequence baseline\nTo provide a simple point of comparison, we also include a linear baseline model. Instead of using a recurrent structure, this model treats the entire input sequence as a single flat vector. All SEQ_LEN feature rows are concatenated into one long input, and the model applies a single fully connected layer to map this vector directly to a predicted next-step flow.\nThis baseline has no notion of temporal order or temporal dependency. It can only learn a weighted combination of the raw features but cannot model how patterns evolve over time. As a result, it serves as a useful reference for evaluating whether the LSTM’s recurrent structure truly provides additional predictive power.\nDespite its simplicity, the linear model is fast to train and helps quantify the value of incorporating temporal dynamics into the prediction task.\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\ninput_dim = len(feature_cols)\nhidden_dim = 64\nnum_layers = 2\n\nclass FlowLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True\n        )\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        # x: [B, L, F]\n        out, _ = self.lstm(x)          # out: [B, L, H]\n        last_hidden = out[:, -1, :]    # [B, H] -&gt; last time step\n        y_pred = self.fc(last_hidden)  # [B, 1]\n        return y_pred.squeeze(-1)      # [B]\n\nmodel = FlowLSTM(input_dim, hidden_dim, num_layers).to(device)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nprint(model)\n\n\n@torch.no_grad()\ndef evaluate_mse_seq(table_name: str) -&gt; float:\n    model.eval()\n    sq_sum = 0.0\n    n = 0\n    for X_batch, y_batch in seq_batch_generator(table_name, batch_size=BATCH_SIZE, seq_len=SEQ_LEN):\n        xb = torch.from_numpy(X_batch).to(device)  # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)  # [B]\n        pred = model(xb)                           # [B]\n        sq_sum += ((pred - yb) ** 2).sum().item()\n        n += len(yb)\n    return sq_sum / n\n\n\n\n\n5. Training\n\n5.1 Linear baseline model\nThe linear baseline model LinearSeqModel is a simple fully connected regressor that ignores temporal order within the input sequence. Each input batch X_batch initially has shape [B, L, F], where B is the batch size, L = SEQ_LEN is the number of time steps, and F is the feature dimension. Before feeding the data into the model, the tensor is reshaped to [B, L * F], so that all time steps in the sequence are concatenated into a single flat feature vector.\nThe model consists of one linear layer nn.Linear(input_dim * seq_len, 1) that maps this flattened sequence to a single scalar prediction. Training uses the Adam optimizer with a learning rate of 1e-3 and mean squared error (MSE) as the loss function. For each epoch, the code iterates over all sequence batches generated from the flow_feat_train table, performs forward and backward passes, and updates the model parameters. The training loop also accumulates the sum of squared errors to compute the training MSE.\nAfter each epoch, the helper function evaluate_mse_linear is called on the flow_feat_test table to compute the test MSE using the same batching logic. This function runs the model in evaluation mode, iterates over all test batches, and returns the average squared error across all samples. Finally, the trained linear model weights are saved to flow_linear_seq_epoch8.pth as the linear baseline checkpoint.\n\nimport torch\nfrom torch import nn\n\nclass LinearSeqModel(nn.Module):\n    def __init__(self, input_dim: int, seq_len: int):\n        \"\"\"\n        input_dim: len(feature_cols)\n        seq_len  : SEQ_LEN\n        \"\"\"\n        super().__init__()\n        self.input_dim = input_dim\n        self.seq_len = seq_len\n        self.fc = nn.Linear(input_dim * seq_len, 1)\n\n    def forward(self, x):\n        \"\"\"\n        x: [B, L*F]\n        \"\"\"\n        y_pred = self.fc(x)          # [B, 1]\n        return y_pred.squeeze(-1)    # [B]\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\ninput_dim = len(feature_cols)\n\nlin_model = LinearSeqModel(input_dim=input_dim, seq_len=SEQ_LEN).to(device)\nlin_loss_fn = nn.MSELoss()\nlin_optimizer = torch.optim.Adam(lin_model.parameters(), lr=1e-3)\n\nprint(lin_model)\n\n\n@torch.no_grad()\ndef evaluate_mse_linear(model, table_name: str, max_cells=None) -&gt; float:\n    model.eval()\n    sq_sum = 0.0\n    n = 0\n\n    for X_batch, y_batch in seq_batch_generator(\n        table_name,\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=max_cells\n    ):\n        xb = torch.from_numpy(X_batch).to(device)    # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)    # [B]\n\n        xb = xb.reshape(xb.shape[0], -1)             # [B, L*F]\n\n        pred = model(xb)                             # [B]\n        sq_sum += ((pred - yb) ** 2).sum().item()\n        n += len(yb)\n\n    return sq_sum / n\n\n\nEPOCHS_LIN = 8\n\nfor epoch in range(1, EPOCHS_LIN + 1):\n    lin_model.train()\n    train_sq_sum = 0.0\n    train_n = 0\n    batch_idx = 0\n\n    for X_batch, y_batch in seq_batch_generator(\n        \"flow_feat_train\",\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=None\n    ):\n        xb = torch.from_numpy(X_batch).to(device)    # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)    # [B]\n\n        # flatten [B, L*F]\n        xb = xb.reshape(xb.shape[0], -1)\n\n        lin_optimizer.zero_grad()\n        pred = lin_model(xb)                         # [B]\n        loss = lin_loss_fn(pred, yb)\n        loss.backward()\n        lin_optimizer.step()\n\n        train_sq_sum += ((pred.detach() - yb) ** 2).sum().item()\n        train_n += len(yb)\n\n        batch_idx += 1\n        if batch_idx % 100 == 0:\n            print(f\"[Linear] Epoch {epoch:02d} | Processed {batch_idx} batches\")\n\n    #  train / test MSE\n    train_mse = train_sq_sum / train_n\n    test_mse = evaluate_mse_linear(lin_model, \"flow_feat_test\", max_cells=None)\n\n    print(f\"[Linear] Epoch {epoch:02d} | Train MSE {train_mse:.4f} | Test MSE {test_mse:.4f}\")\n\ntorch.save(\n    lin_model.state_dict(),\n    \"/content/drive/MyDrive/MOBILITY/flow_linear_seq_epoch8.pth\"\n)\nprint(\"Saved linear baseline model.\")\n\n\n\n\n5.2 LSTM training and checkpointing\nThe second training loop trains the LSTM-based sequence model defined earlier in the notebook. In contrast to the linear baseline, the LSTM takes input batches of shape [B, L, F] directly and processes the sequence along the time dimension, maintaining a hidden state to capture temporal dependencies.\nThe helper function evaluate_mse_seq mirrors the linear evaluation function, but it calls the sequence model directly on tensors of shape [B, L, F] without flattening. It runs the model in evaluation mode, iterates over all batches from the specified DuckDB table, and returns the mean squared error over all samples.\nThe main training loop runs for EPOCHS = 8. For each epoch, the model is set to training mode, and batches are drawn from the flow_feat_train table using seq_batch_generator. For every batch, the code moves the data to the selected device (CPU or GPU), performs a forward pass through the LSTM, computes the MSE loss, backpropagates the gradients, and updates the parameters with the chosen optimizer. The loop accumulates the sum of squared errors to compute the epoch-level training MSE, and every 100 batches it prints a progress message.\nAt the end of each epoch, evaluate_mse_seq is called on the flow_feat_test table to obtain the test MSE. The code then saves a checkpoint file under checkpoints_lstm/, containing the current epoch index, the model and optimizer state dictionaries, and the train/test MSE values. This allows intermediate models to be restored later if needed. After the final epoch, the model’s state dictionary is also saved separately to flow_lstm_final.pth as the final LSTM checkpoint.\n\nimport os\n\n# checkpoint\nckpt_dir = \"/content/drive/MyDrive/MOBILITY/checkpoints_lstm\"\nos.makedirs(ckpt_dir, exist_ok=True)\n\n# ----- training loop ----- checkpoint\nEPOCHS = 8\n\n@torch.no_grad()\ndef evaluate_mse_seq(table_name: str, max_cells=None) -&gt; float:\n    model.eval()\n    sq_sum = 0.0\n    n = 0\n    for X_batch, y_batch in seq_batch_generator(\n        table_name,\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=max_cells\n    ):\n        xb = torch.from_numpy(X_batch).to(device)  # [B, L, F]\n        yb = torch.from_numpy(y_batch).to(device)  # [B]\n        pred = model(xb)                           # [B]\n        sq_sum += ((pred - yb) ** 2).sum().item()\n        n += len(yb)\n    return sq_sum / n\n\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    train_sq_sum = 0.0\n    train_n = 0\n\n    batch_idx = 0\n\n    # Here you can set max_cells for debugging; set to None for full data\n    for X_batch, y_batch in seq_batch_generator(\n        \"flow_feat_train\",\n        batch_size=BATCH_SIZE,\n        seq_len=SEQ_LEN,\n        max_cells=None\n    ):\n        xb = torch.from_numpy(X_batch).to(device)\n        yb = torch.from_numpy(y_batch).to(device)\n\n        optimizer.zero_grad()\n        pred = model(xb)\n        loss = loss_fn(pred, yb)\n        loss.backward()\n        optimizer.step()\n\n        train_sq_sum += ((pred.detach() - yb) ** 2).sum().item()\n        train_n += len(yb)\n\n        batch_idx += 1\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch:02d} | Processed {batch_idx} batches so far\")\n\n    train_mse = train_sq_sum / train_n\n    test_mse = evaluate_mse_seq(\"flow_feat_test\", max_cells=None)\n\n    print(f\"Epoch {epoch:02d} | Train MSE {train_mse:.4f} | Test MSE {test_mse:.4f}\")\n\n    ckpt_path = os.path.join(ckpt_dir, f\"flow_lstm_epoch{epoch:02d}.pth\")\n    torch.save(\n        {\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"train_mse\": train_mse,\n            \"test_mse\": test_mse,\n        },\n        ckpt_path,\n    )\n    print(f\"Saved checkpoint to {ckpt_path}\")\n\nfinal_path = \"/content/drive/MyDrive/MOBILITY/flow_lstm_final.pth\"\ntorch.save(model.state_dict(), final_path)\nprint(f\"Saved final model to {final_path}\")\n\n\n\n\n\n6. Spatial visualization\nTo understand where and when the models perform well, we visualize predictions across the spatial grid. The idea is to fix a specific day and one or more time-of-day slots, reconstruct the historical sequences for each grid cell at those times, run the trained model, and compare its predictions with the true flows.\nThe visualization works in two modes:\n\nSingle time slice\n\nChoose a day and an eight-step window within that day.\nFor the final time step of the window, identify all grid cells that have a valid next-step flow.\nFor each of these cells, retrieve the preceding SEQ_LEN records to form a complete history.\nAfter running the model, create a spatial plot showing:\n\nthe true next-step flow,\nthe model’s predicted flow,\nand the absolute error.\n\n\n\n\nd0 = 64\n\nimport matplotlib.pyplot as plt\n\nstart_t = 16\nwindow_len = 8\nend_t = start_t + window_len - 1\n\nwindow_df = con.execute(\"\"\"\n    SELECT\n        x,\n        y,\n        (time_index % 48) AS t,\n        flow\n    FROM flow_feat_test\n    WHERE (time_index / 48)::INT = ?\n      AND (time_index % 48) BETWEEN ? AND ?\n\"\"\", [d0, start_t, end_t]).fetchdf()\n\nprint(window_df.head())\nprint(\"records have =\", len(window_df))\n\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\nt_plot = end_t\n\nsnap_true = con.execute(f\"\"\"\n    SELECT\n        x,\n        y,\n        flow_next AS target,\n        time_index\n    FROM flow_feat_test\n    WHERE (time_index / 48)::INT = ?\n      AND (time_index % 48) = ?;\n\"\"\", [d0, t_plot]).fetchdf()\n\nprint(\"cells at this time step:\", len(snap_true))\n\n\n\nX_list, target_list, x_list, y_list = [], [], [], []\n\nfor idx, row in snap_true.iterrows():\n    x0 = int(row[\"x\"])\n    y0 = int(row[\"y\"])\n    T  = int(row[\"time_index\"])\n    y_true = float(row[\"target\"])\n\n    seq_df = con.execute(f\"\"\"\n        SELECT time_index, {\", \".join(feature_cols)}\n        FROM (\n            SELECT time_index, {\", \".join(feature_cols)}\n            FROM flow_feat_test\n            WHERE x = ? AND y = ?\n              AND (time_index / 48)::INT = ?\n              AND time_index &lt;= ?\n            ORDER BY time_index DESC\n            LIMIT ?\n        )\n        ORDER BY time_index;\n    \"\"\", [x0, y0, d0, T, SEQ_LEN]).fetchdf()\n\n    if len(seq_df) &lt; SEQ_LEN:\n        continue\n\n    feats = seq_df[feature_cols].to_numpy(dtype=\"float32\")\n    feats_norm = (feats - mean) / std\n\n    X_list.append(feats_norm)\n    target_list.append(y_true)\n    x_list.append(x0)\n    y_list.append(y0)\n\nprint(\"usable cells with full 8-step history:\", len(X_list))\n\nX = np.stack(X_list).astype(\"float32\")       # [N, L, F]\ny_true_arr = np.array(target_list, dtype=\"float32\")\n\nmodel.eval()\nwith torch.no_grad():\n    xb = torch.from_numpy(X).to(device)     # [N, L, F]\n    pred_arr = model(xb).cpu().numpy()      # [N]\n\n\nsnap = pd.DataFrame({\n    \"x\": x_list,\n    \"y\": y_list,\n    \"flow_true\": y_true_arr,     # true next-step flow\n    \"flow_pred\": pred_arr,       # LSTM predicted next-step flow\n})\nsnap[\"err_abs\"] = np.abs(snap[\"flow_pred\"] - snap[\"flow_true\"])\n\n\nimport matplotlib.pyplot as plt\n\nvmin = np.percentile(snap[\"flow_true\"], 5)\nvmax = np.percentile(snap[\"flow_true\"], 95)\n\nerr_max = np.percentile(snap[\"err_abs\"], 95)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n\nax = axes[0]\nsc0 = ax.scatter(\n    snap[\"x\"], snap[\"y\"],\n    c=snap[\"flow_true\"],\n    s=10,\n    cmap=\"plasma\",\n    vmin=vmin,\n    vmax=vmax\n)\nax.set_title(\"True next-step flow\")\nax.set_aspect(\"equal\")\nax.grid(False)\nplt.colorbar(sc0, ax=ax, label=\"flow_true\")\n\nax = axes[1]\nsc1 = ax.scatter(\n    snap[\"x\"], snap[\"y\"],\n    c=snap[\"flow_pred\"],\n    s=10,\n    cmap=\"plasma\",\n    vmin=vmin,\n    vmax=vmax\n)\nax.set_title(\"LSTM predicted next-step flow\")\nax.set_aspect(\"equal\")\nax.grid(False)\nplt.colorbar(sc1, ax=ax, label=\"flow_pred\")\n\nax = axes[2]\nsc2 = ax.scatter(\n    snap[\"x\"], snap[\"y\"],\n    c=snap[\"err_abs\"],\n    s=10,\n    cmap=\"magma\",\n    vmin=0,\n    vmax=err_max\n)\nax.set_title(\"|pred − true|\")\nax.set_aspect(\"equal\")\nax.grid(False)\nplt.colorbar(sc2, ax=ax, label=\"abs error\")\n\nfor ax in axes:\n    ax.set_xlabel(\"x\")\naxes[0].set_ylabel(\"y\")\n\nminutes = t_plot * 30\nhh = minutes // 60\nmm = minutes % 60\nplt.suptitle(f\"Day {d0}, t={t_plot} (~{hh:02d}:{mm:02d}) – next-step flow\", fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\n\n\nMultiple time slices\n\nSelect several time-of-day indices on the same day.\nRepeat the above process for each time, stacking the resulting maps side-by-side.\nThe first row displays true flows, the second row shows predictions, and the third row shows absolute errors.\nShared color scales allow visual comparison across time.\n\n\nThese visualizations reveal spatial heterogeneity in both mobility patterns and model performance. They help identify when the model struggles, which regions tend to be more predictable, and how flow dynamics evolve over the day.\n\nd0 = 64\n\nt_list = [8, 12, 16, 20, 24, 28]\n\nsnap_list = []\n\nmodel.eval()\n\nfor t_plot in t_list:\n    snap_true = con.execute(\"\"\"\n        SELECT\n            x,\n            y,\n            flow_next AS target,\n            time_index\n        FROM flow_feat_test\n        WHERE (time_index / 48)::INT = ?\n          AND (time_index % 48) = ?;\n    \"\"\", [d0, t_plot]).fetchdf()\n\n    X_list, target_list, x_list, y_list = [], [], [], []\n\n    for idx, row in snap_true.iterrows():\n        x0 = int(row[\"x\"])\n        y0 = int(row[\"y\"])\n        T  = int(row[\"time_index\"])\n        y_true = float(row[\"target\"])\n\n        seq_df = con.execute(f\"\"\"\n            SELECT time_index, {\", \".join(feature_cols)}\n            FROM (\n                SELECT time_index, {\", \".join(feature_cols)}\n                FROM flow_feat_test\n                WHERE x = ? AND y = ?\n                  AND (time_index / 48)::INT = ?\n                  AND time_index &lt;= ?\n                ORDER BY time_index DESC\n                LIMIT ?\n            )\n            ORDER BY time_index;\n        \"\"\", [x0, y0, d0, T, SEQ_LEN]).fetchdf()\n\n        if len(seq_df) &lt; SEQ_LEN:\n            continue\n\n        feats = seq_df[feature_cols].to_numpy(dtype=\"float32\")\n        feats_norm = (feats - mean) / std\n\n        X_list.append(feats_norm)\n        target_list.append(y_true)\n        x_list.append(x0)\n        y_list.append(y0)\n\n    if len(X_list) == 0:\n        print(f\"t={t_plot} no enough\")\n        continue\n\n    X = np.stack(X_list).astype(\"float32\")\n    y_true_arr = np.array(target_list, dtype=\"float32\")\n\n    with torch.no_grad():\n        xb = torch.from_numpy(X).to(device)\n        y_pred_arr = model(xb).cpu().numpy()\n\n    snap = pd.DataFrame({\n        \"x\": x_list,\n        \"y\": y_list,\n        \"flow_true\": y_true_arr,\n        \"flow_pred\": y_pred_arr,\n    })\n    snap[\"err_abs\"] = np.abs(snap[\"flow_pred\"] - snap[\"flow_true\"])\n    snap[\"t_plot\"] = t_plot\n\n    snap_list.append(snap)\n\nprint(\"usable time points\", len(snap_list))\n\n\nall_snap = pd.concat(snap_list, ignore_index=True)\nvmin = np.percentile(all_snap[\"flow_true\"], 5)\nvmax = np.percentile(all_snap[\"flow_true\"], 95)\nerr_max = np.percentile(all_snap[\"err_abs\"], 95)\n\nn_times = len(snap_list)\nsnap_list_sorted = sorted(snap_list, key=lambda s: s[\"t_plot\"].iloc[0])\n\nfig, axes = plt.subplots(\n    3, n_times,\n    figsize=(3.0 * n_times, 8),\n    sharex=True, sharey=True\n)\n\nsc_true = None\nsc_err = None\n\nfor j, snap in enumerate(snap_list_sorted):\n    t_plot = snap[\"t_plot\"].iloc[0]\n\n    ax = axes[0, j]\n    sc_true = ax.scatter(\n        snap[\"x\"], snap[\"y\"],\n        c=snap[\"flow_true\"],\n        s=5,\n        cmap=\"plasma\",\n        vmin=vmin, vmax=vmax\n    )\n    ax.set_title(f\"t={t_plot} True\", fontsize=10)\n    ax.set_aspect(\"equal\")\n    ax.grid(False)\n\n    ax = axes[1, j]\n    ax.scatter(\n        snap[\"x\"], snap[\"y\"],\n        c=snap[\"flow_pred\"],\n        s=5,\n        cmap=\"plasma\",\n        vmin=vmin, vmax=vmax\n    )\n    ax.set_title(f\"t={t_plot} Pred\", fontsize=10)\n    ax.set_aspect(\"equal\")\n    ax.grid(False)\n\n    ax = axes[2, j]\n    sc_err = ax.scatter(\n        snap[\"x\"], snap[\"y\"],\n        c=snap[\"err_abs\"],\n        s=5,\n        cmap=\"magma\",\n        vmin=0, vmax=err_max\n    )\n    ax.set_title(f\"t={t_plot} |Pred−True|\", fontsize=10)\n    ax.set_aspect(\"equal\")\n    ax.grid(False)\n\nfor j in range(n_times):\n    axes[2, j].set_xlabel(\"x\")\nfor i in range(3):\n    axes[i, 0].set_ylabel(\"y\")\n\nplt.subplots_adjust(\n    left=0.06, right=0.9,\n    top=0.9, bottom=0.08,\n    wspace=0.05, hspace=0.12\n)\n\ncbar1 = fig.colorbar(\n    sc_true,\n    ax=axes[0:2, -1],\n    fraction=0.046,\n    pad=0.02\n)\ncbar1.set_label(\"flow\")\n\ncbar2 = fig.colorbar(\n    sc_err,\n    ax=axes[2, -1],\n    fraction=0.046,\n    pad=0.04\n)\ncbar2.set_label(\"abs error\")\n\nplt.suptitle(f\"Day {d0} – 6 time steps: True / Pred / Error\", fontsize=14)\nplt.show()\n\n\n\n\nConclusion\nThe results demonstrate that temporal modeling is essential for predicting grid-based mobility flows. The linear baseline, which treats the input sequence as a single flattened vector, fails to capture temporal structure: its test MSE is more than two orders of magnitude worse than that of the LSTM. In contrast, the LSTM achieves stable performance (train MSE ≈ 3.79, test MSE ≈ 4.15), indicating that it successfully learns short-term temporal dependencies in the flow dynamics.\nSpatial visualizations further confirm this pattern. Across six representative time steps on Day 64, the LSTM reproduces the broad spatial structure of flow, accurately matching both low-flow regions and the major concentration zones. Prediction errors are primarily concentrated in high-intensity areas—locations where flow is both larger in magnitude and more volatile. This suggests that the LSTM captures routine, low-variance mobility well, while sudden spikes or irregular flow patterns remain harder to model.\nOverall, the experiment shows that: 1. Temporal information is crucial for mobility forecasting. 2. Even a relatively small LSTM (hidden size 64, two layers) is sufficient to outperform non-temporal baselines by a wide margin. 3. The remaining errors are spatially structured, pointing toward opportunities for future extensions such as attention mechanisms or spatial graph models.\nThe LSTM therefore provides a strong foundation for next-step mobility prediction, and the spatial evaluation highlights where more advanced models may yield additional gains."
  }
]